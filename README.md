# Support_Vector_Regression
> Support Vector Regression (SVR) is an application of the Support Vector Machine (SVM) algorithm for regression tasks. Unlike traditional regression models that aim to minimize the error, SVR seeks to find a function that predicts within a certain margin of tolerance.

> SVR introduces a margin of tolerance (ğœ–) within which predictions are considered acceptable. Data points lying outside the ğœ–-margin are called support vectors, as they determine the regression line.

## Advantages of SVR
> + Effective with High Dimensions:
Handles high-dimensional data efficiently, especially with kernel tricks.

> + Robust to Outliers:
Focuses on data within the ğœ–-margin, ignoring small deviations.

> + Non-Linear Relationships:
Models complex, non-linear relationships through kernels.

## Disadvantages of SVR
> + Computational Complexity:
Training can be slow for large datasets due to quadratic optimization.
> + Scalability:
May struggle with very large datasets.
> + Parameter Tuning:
Requires careful tuning of hyperparameters (ğœ–, and kernel parameters).

### Note:
Ordinary Least Squares helps to minimize the error between the actual value and predicted values over the regression line. That is squaring the difference between the actual value and predicted value. But in SVR, a tube is spread over the data. 

[Click here to see case for ordinary least squares and SVR](https://ibb.co/Y8fytmp)
